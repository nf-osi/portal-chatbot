# Scripts for Creating Benchmark Datasets and Evaluation 

## General Help Dataset

This dataset is used in Quality Assurance of our deployed portal chatbot, following rigorous best practices... TODO: explain more here.

### Get content from NF docs with crawler

This contains a Scrapy spider that crawls all pages under the [public NF help docs](https://help.nf.synapse.org/NFdocs/) section and converts each page into a Markdown document. The Markdown files are saved in an output directory for further use (but not included in this repository). This is used for creating the evaluation dataset.

#### Overview

The spider starts at the base URL (`https://help.nf.synapse.org/NFdocs/`), follows all internal links within the NFdocs section, converts the HTML content of each page to Markdown using the [`markdownify`](https://github.com/matthewwithanm/python-markdownify) library, and saves the output as individual Markdown files.

#### Requirements

- Python 3.x
- [Scrapy](https://scrapy.org/)
- [markdownify](https://github.com/matthewwithanm/python-markdownify)

#### Run crawler

```
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
pip install scrapy markdownify
scrapy runspider nfdocs_spider.py
```

Now check that a directory `output_markdown` has been created locally and contains the Markdown files (again, these are git-ignored).

#### BATCH_API for synthetic dataset with LLM

This section explains how to generate a synthetic dataset using a large language model (LLM) to produce multiple-choice questions for evaluating the NF portal chatbot.

**Dataset Structure and Associated Prompt/Schema:**

- **Dataset Structure:**  
  The synthetic dataset is expected to be a CSV table where each row represents a multiple-choice question. Each entry must include:
  - **question:** A string intended to reveal potential inaccuracies or common misconceptions.
  - **mc1_targets:** A dictionary with two keys:
    - **choices:** An array containing 4 to 5 answer choice strings.
    - **labels:** An array of integers (0 or 1) where exactly one entry is 1 (indicating the correct answer).

- **Prompt and Schema:**  
  The process uses the `generate_dataset.py` script to:
  - Load and combine Markdown files from `benchmark/general-help/output_markdown` (which contains the NF documentation).
  - Load the QA schema from `benchmark/general-help/qa-schema.json` (modified to support the multiple-choice structure).
  - Construct a system prompt and a user prompt that provide context and instructions to the LLM. The system prompt details the task, while the user prompt supplies the combined documentation and the schema.
  - Output a JSONL file with a properly structured API call for the LLM.

**Batch API Usage:**

- **OpenAI Model and Endpoint:**  
  The JSONL entry created by the script is intended for submission to a batch API endpoint (`/v1/chat/completions`). The model used is `gpt-4o-mini` (or another specified model) which processes the prompts in batches.
  
- **Process Flow:**  
  The batch API receives the JSONL entry and returns a CSV table as a response. This CSV table contains the synthetic multiple-choice questions formatted according to the schema.

**Human Validation of Synthetic Dataset:**

- **Review Process:**  
  After the synthetic dataset is generated by the LLM, human reviewers:
  - Check that each question is coherent and relevant.
  - Verify that the answer choices are logical and that the correct answer is correctly indicated.
  
- **Feedback Loop:**  
  Any issues identified during the review are fed back to improve both the prompt instructions and the overall generation process, ensuring that future iterations produce higher quality datasets.

#### Run BATCH API

```
python benchmark/general-help/generate_dataset.py
```

TODO:
- Explain dataset structure and associated prompt/schema here
- Explain batch API usage here (including OpenAI model used)
- Explain human validation of synthetic dataset
